{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Torch models (Step by Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from skorch import NeuralNet, callbacks, NeuralNetClassifier\n",
    "import numpy as np\n",
    "from evaluator import Evaluator\n",
    "from trainer import SkorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: SkorchTrainer is a class in \"trainer.py\" which is responsible for training torch models. The presented NeuralNetClassifier classes are only showcases and the actual declarations are defined inside the trainer itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.loadtxt('data/transform_data.csv', skiprows=1, delimiter=',')\n",
    "X, y = Data[:, :-1], Data[:, -1].astype('int')\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state = 0)\n",
    "trainer = SkorchTrainer(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Training Conv1D using skorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training CONV1D\n",
      "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -----------  ------------  -----------  ------------  ------\n",
      "      1       \u001b[36m0.3569\u001b[0m        \u001b[32m1.0974\u001b[0m       \u001b[35m0.3569\u001b[0m        \u001b[31m1.0971\u001b[0m  2.4160\n",
      "      2       0.3569        \u001b[32m1.0973\u001b[0m       0.3569        \u001b[31m1.0971\u001b[0m  1.7270\n",
      "      3       0.3569        \u001b[32m1.0973\u001b[0m       0.3569        \u001b[31m1.0970\u001b[0m  1.8340\n",
      "      4       0.3569        \u001b[32m1.0972\u001b[0m       0.3569        \u001b[31m1.0969\u001b[0m  1.7420\n",
      "      5       0.3569        \u001b[32m1.0971\u001b[0m       0.3569        \u001b[31m1.0968\u001b[0m  1.7680\n",
      "      6       0.3569        \u001b[32m1.0968\u001b[0m       0.3569        \u001b[31m1.0962\u001b[0m  1.7540\n",
      "      7       0.3569        \u001b[32m1.0959\u001b[0m       0.3569        \u001b[31m1.0939\u001b[0m  1.6560\n",
      "      8       \u001b[36m0.3817\u001b[0m        \u001b[32m1.0901\u001b[0m       \u001b[35m0.4233\u001b[0m        \u001b[31m1.0773\u001b[0m  1.7520\n",
      "      9       \u001b[36m0.4087\u001b[0m        \u001b[32m1.0718\u001b[0m       \u001b[35m0.4236\u001b[0m        \u001b[31m1.0555\u001b[0m  1.7200\n",
      "     10       0.4056        \u001b[32m1.0633\u001b[0m       \u001b[35m0.4290\u001b[0m        \u001b[31m1.0521\u001b[0m  1.7400\n",
      "     11       0.4087        \u001b[32m1.0611\u001b[0m       0.4283        \u001b[31m1.0506\u001b[0m  1.7290\n",
      "     12       0.4083        \u001b[32m1.0605\u001b[0m       0.4273        \u001b[31m1.0498\u001b[0m  1.6560\n"
     ]
    }
   ],
   "source": [
    "trainer.train_conv1d(Conv1dText,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Training Conv2D using skorch*\n",
    "Conv2D network which is called CBR_Network in \"models.py\" drops the last feature of tweet vectors to reshape the tensor into a squeared 7x7 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training CONV2D\n",
      "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -----------  ------------  -----------  ------------  ------\n",
      "      1       \u001b[36m0.3843\u001b[0m        \u001b[32m1.0861\u001b[0m       \u001b[35m0.4183\u001b[0m        \u001b[31m1.0661\u001b[0m  1.5470\n",
      "      2       \u001b[36m0.4128\u001b[0m        \u001b[32m1.0635\u001b[0m       \u001b[35m0.4446\u001b[0m        \u001b[31m1.0490\u001b[0m  1.2140\n",
      "      3       \u001b[36m0.4311\u001b[0m        \u001b[32m1.0520\u001b[0m       \u001b[35m0.4512\u001b[0m        \u001b[31m1.0401\u001b[0m  1.2880\n",
      "      4       \u001b[36m0.4443\u001b[0m        \u001b[32m1.0445\u001b[0m       0.4459        \u001b[31m1.0390\u001b[0m  1.2800\n",
      "      5       \u001b[36m0.4506\u001b[0m        \u001b[32m1.0367\u001b[0m       0.4502        \u001b[31m1.0364\u001b[0m  1.2480\n",
      "      6       \u001b[36m0.4623\u001b[0m        \u001b[32m1.0329\u001b[0m       0.4512        1.0423  1.2150\n",
      "      7       \u001b[36m0.4708\u001b[0m        \u001b[32m1.0281\u001b[0m       0.4509        \u001b[31m1.0346\u001b[0m  1.2580\n",
      "      8       \u001b[36m0.4770\u001b[0m        \u001b[32m1.0234\u001b[0m       \u001b[35m0.4618\u001b[0m        \u001b[31m1.0345\u001b[0m  1.2810\n",
      "      9       \u001b[36m0.4982\u001b[0m        \u001b[32m1.0149\u001b[0m       0.4539        \u001b[31m1.0327\u001b[0m  1.2900\n",
      "     10       0.4947        \u001b[32m1.0143\u001b[0m       0.4529        \u001b[31m1.0317\u001b[0m  1.3090\n"
     ]
    }
   ],
   "source": [
    "trainer.train_conv2d(CBR_Network,49,(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Training RNN using skorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 2\n",
    "seq_length = X_train.shape[1]//input_size\n",
    "hidden_size = 15\n",
    "num_layers = 3\n",
    "num_classes = 3\n",
    "model = NeuralNetClassifier(module = RecurrentNN,\n",
    "                            module__input_size = input_size,\n",
    "                            module__hidden_size = hidden_size,\n",
    "                            module__num_layers = num_layers,\n",
    "                            module__seq_length  = seq_length,\n",
    "                            module__num_classes = num_classes,\n",
    "                            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                            criterion = nn.CrossEntropyLoss,\n",
    "                            optimizer = optim.SGD,\n",
    "                            lr = 0.008,\n",
    "                            optimizer__momentum=0.9,\n",
    "                            callbacks=[('tr_acc', callbacks.EpochScoring('accuracy',\n",
    "                                                                         lower_is_better=False,\n",
    "                                                                         on_train=True,\n",
    "                                                                         name='train_acc'))],\n",
    "                            batch_size = 2000,\n",
    "                            max_epochs= 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Tensors out of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training RNN\n",
      "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -----------  ------------  -----------  ------------  ------\n",
      "      1       \u001b[36m0.3380\u001b[0m        \u001b[32m1.1014\u001b[0m       \u001b[35m0.3499\u001b[0m        \u001b[31m1.1012\u001b[0m  1.1320\n",
      "      2       \u001b[36m0.3425\u001b[0m        \u001b[32m1.0967\u001b[0m       0.3453        \u001b[31m1.0942\u001b[0m  0.4240\n",
      "      3       \u001b[36m0.3615\u001b[0m        \u001b[32m1.0907\u001b[0m       \u001b[35m0.4100\u001b[0m        \u001b[31m1.0881\u001b[0m  0.4850\n",
      "      4       \u001b[36m0.4258\u001b[0m        \u001b[32m1.0835\u001b[0m       \u001b[35m0.4233\u001b[0m        \u001b[31m1.0820\u001b[0m  0.4310\n",
      "      5       \u001b[36m0.4296\u001b[0m        \u001b[32m1.0779\u001b[0m       \u001b[35m0.4253\u001b[0m        \u001b[31m1.0761\u001b[0m  0.4900\n",
      "      6       \u001b[36m0.4360\u001b[0m        \u001b[32m1.0722\u001b[0m       \u001b[35m0.4280\u001b[0m        \u001b[31m1.0708\u001b[0m  0.4400\n",
      "      7       \u001b[36m0.4401\u001b[0m        \u001b[32m1.0668\u001b[0m       \u001b[35m0.4309\u001b[0m        \u001b[31m1.0660\u001b[0m  0.4890\n",
      "      8       \u001b[36m0.4409\u001b[0m        \u001b[32m1.0620\u001b[0m       0.4286        \u001b[31m1.0613\u001b[0m  0.4500\n",
      "      9       \u001b[36m0.4417\u001b[0m        \u001b[32m1.0573\u001b[0m       \u001b[35m0.4316\u001b[0m        \u001b[31m1.0569\u001b[0m  0.5300\n",
      "     10       \u001b[36m0.4448\u001b[0m        \u001b[32m1.0530\u001b[0m       \u001b[35m0.4326\u001b[0m        \u001b[31m1.0529\u001b[0m  0.4240\n",
      "     11       \u001b[36m0.4453\u001b[0m        \u001b[32m1.0490\u001b[0m       \u001b[35m0.4353\u001b[0m        \u001b[31m1.0492\u001b[0m  0.4540\n",
      "     12       0.4448        \u001b[32m1.0453\u001b[0m       0.4333        \u001b[31m1.0459\u001b[0m  0.5270\n",
      "     13       \u001b[36m0.4466\u001b[0m        \u001b[32m1.0418\u001b[0m       0.4343        \u001b[31m1.0429\u001b[0m  0.4410\n",
      "     14       \u001b[36m0.4493\u001b[0m        \u001b[32m1.0388\u001b[0m       0.4349        \u001b[31m1.0402\u001b[0m  0.5040\n",
      "     15       \u001b[36m0.4501\u001b[0m        \u001b[32m1.0360\u001b[0m       0.4339        \u001b[31m1.0378\u001b[0m  0.4460\n",
      "     16       \u001b[36m0.4511\u001b[0m        \u001b[32m1.0336\u001b[0m       \u001b[35m0.4356\u001b[0m        \u001b[31m1.0358\u001b[0m  0.5190\n",
      "     17       \u001b[36m0.4537\u001b[0m        \u001b[32m1.0315\u001b[0m       \u001b[35m0.4376\u001b[0m        \u001b[31m1.0340\u001b[0m  0.5320\n",
      "     18       \u001b[36m0.4551\u001b[0m        \u001b[32m1.0296\u001b[0m       \u001b[35m0.4389\u001b[0m        \u001b[31m1.0326\u001b[0m  0.4690\n",
      "     19       \u001b[36m0.4572\u001b[0m        \u001b[32m1.0281\u001b[0m       0.4376        \u001b[31m1.0314\u001b[0m  0.5720\n",
      "     20       \u001b[36m0.4581\u001b[0m        \u001b[32m1.0268\u001b[0m       \u001b[35m0.4402\u001b[0m        \u001b[31m1.0305\u001b[0m  0.5230\n",
      "     21       \u001b[36m0.4605\u001b[0m        \u001b[32m1.0257\u001b[0m       0.4402        \u001b[31m1.0297\u001b[0m  0.5060\n",
      "     22       \u001b[36m0.4631\u001b[0m        \u001b[32m1.0249\u001b[0m       \u001b[35m0.4419\u001b[0m        \u001b[31m1.0292\u001b[0m  0.4730\n",
      "     23       0.4628        \u001b[32m1.0241\u001b[0m       \u001b[35m0.4446\u001b[0m        \u001b[31m1.0287\u001b[0m  0.4370\n",
      "     24       \u001b[36m0.4637\u001b[0m        \u001b[32m1.0235\u001b[0m       \u001b[35m0.4472\u001b[0m        \u001b[31m1.0284\u001b[0m  0.5550\n",
      "     25       \u001b[36m0.4653\u001b[0m        \u001b[32m1.0230\u001b[0m       \u001b[35m0.4489\u001b[0m        \u001b[31m1.0282\u001b[0m  0.4590\n",
      "     26       0.4648        \u001b[32m1.0226\u001b[0m       0.4485        \u001b[31m1.0280\u001b[0m  0.4570\n",
      "     27       0.4648        \u001b[32m1.0222\u001b[0m       0.4482        \u001b[31m1.0278\u001b[0m  0.5080\n",
      "     28       0.4650        \u001b[32m1.0218\u001b[0m       0.4482        \u001b[31m1.0276\u001b[0m  0.4670\n",
      "     29       0.4653        \u001b[32m1.0215\u001b[0m       \u001b[35m0.4492\u001b[0m        \u001b[31m1.0274\u001b[0m  0.5430\n",
      "     30       \u001b[36m0.4663\u001b[0m        \u001b[32m1.0212\u001b[0m       \u001b[35m0.4505\u001b[0m        \u001b[31m1.0272\u001b[0m  0.4750\n",
      "     31       \u001b[36m0.4678\u001b[0m        \u001b[32m1.0208\u001b[0m       0.4492        \u001b[31m1.0270\u001b[0m  0.4810\n",
      "     32       \u001b[36m0.4679\u001b[0m        \u001b[32m1.0205\u001b[0m       0.4485        \u001b[31m1.0267\u001b[0m  0.4600\n",
      "     33       \u001b[36m0.4688\u001b[0m        \u001b[32m1.0202\u001b[0m       0.4482        \u001b[31m1.0264\u001b[0m  0.4430\n",
      "     34       \u001b[36m0.4689\u001b[0m        \u001b[32m1.0199\u001b[0m       0.4495        \u001b[31m1.0260\u001b[0m  0.5280\n",
      "     35       \u001b[36m0.4692\u001b[0m        \u001b[32m1.0195\u001b[0m       0.4492        \u001b[31m1.0257\u001b[0m  0.4350\n",
      "     36       \u001b[36m0.4694\u001b[0m        \u001b[32m1.0192\u001b[0m       0.4472        \u001b[31m1.0253\u001b[0m  0.5310\n",
      "     37       \u001b[36m0.4695\u001b[0m        \u001b[32m1.0188\u001b[0m       0.4475        \u001b[31m1.0249\u001b[0m  0.4250\n",
      "     38       \u001b[36m0.4702\u001b[0m        \u001b[32m1.0185\u001b[0m       0.4482        \u001b[31m1.0244\u001b[0m  0.5010\n",
      "     39       \u001b[36m0.4710\u001b[0m        \u001b[32m1.0181\u001b[0m       0.4475        \u001b[31m1.0240\u001b[0m  0.4270\n",
      "     40       \u001b[36m0.4711\u001b[0m        \u001b[32m1.0177\u001b[0m       0.4482        \u001b[31m1.0235\u001b[0m  0.5650\n"
     ]
    }
   ],
   "source": [
    "input_size = 2\n",
    "trainer.train_rnn(RecurrentNN,params={\n",
    "    'input_size': input_size,\n",
    "    'seq_length': X_train.shape[1]//input_size,\n",
    "    'hidden_size': 15,\n",
    "    'num_layers': 3,\n",
    "    'num_classes': 3\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNN using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.09577\n",
      "Epoch: 1, Training Loss: 1.08306\n",
      "Epoch: 2, Training Loss: 1.06917\n",
      "Epoch: 3, Training Loss: 1.05581\n",
      "Epoch: 4, Training Loss: 1.04332\n",
      "Epoch: 5, Training Loss: 1.03185\n",
      "Epoch: 6, Training Loss: 1.02154\n",
      "Epoch: 7, Training Loss: 1.01255\n",
      "Epoch: 8, Training Loss: 1.00491\n",
      "Epoch: 9, Training Loss: 0.99853\n",
      "Epoch: 10, Training Loss: 0.99324\n",
      "Epoch: 11, Training Loss: 0.98883\n",
      "Epoch: 12, Training Loss: 0.98512\n",
      "Epoch: 13, Training Loss: 0.98194\n",
      "Epoch: 14, Training Loss: 0.97917\n",
      "Epoch: 15, Training Loss: 0.97674\n",
      "Epoch: 16, Training Loss: 0.97459\n",
      "Epoch: 17, Training Loss: 0.97264\n",
      "Epoch: 18, Training Loss: 0.97087\n",
      "Epoch: 19, Training Loss: 0.96923\n",
      "Epoch: 20, Training Loss: 0.96769\n",
      "Epoch: 21, Training Loss: 0.96622\n",
      "Epoch: 22, Training Loss: 0.96480\n",
      "Epoch: 23, Training Loss: 0.96342\n",
      "Epoch: 24, Training Loss: 0.96206\n",
      "Epoch: 25, Training Loss: 0.96071\n",
      "Epoch: 26, Training Loss: 0.95937\n",
      "Epoch: 27, Training Loss: 0.95803\n",
      "Epoch: 28, Training Loss: 0.95667\n",
      "Epoch: 29, Training Loss: 0.95531\n",
      "Epoch: 30, Training Loss: 0.95394\n",
      "Epoch: 31, Training Loss: 0.95256\n",
      "Epoch: 32, Training Loss: 0.95115\n",
      "Epoch: 33, Training Loss: 0.94974\n",
      "Epoch: 34, Training Loss: 0.94830\n",
      "Epoch: 35, Training Loss: 0.94684\n",
      "Epoch: 36, Training Loss: 0.94537\n",
      "Epoch: 37, Training Loss: 0.94388\n",
      "Epoch: 38, Training Loss: 0.94236\n",
      "Epoch: 39, Training Loss: 0.94082\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) \n",
    "input_size = 2\n",
    "seq_length = X_train.shape[1]//input_size\n",
    "hidden_size = 15\n",
    "num_layers = 3\n",
    "num_classes = 3\n",
    "learning_rate = 0.008\n",
    "epoch_number = 40\n",
    "batch_size = 2000\n",
    "iteration = X_train.shape[0] // batch_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RNN = RecurrentNN(input_size, hidden_size, num_layers, seq_length, num_classes).to(device)\n",
    "optimizer = optim.SGD(RNN.parameters(), lr = learning_rate, momentum = 0.9 )\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_errors = []\n",
    "for i in range(epoch_number):\n",
    "    for j in range(iteration):\n",
    "        \n",
    "        # Dividing batches\n",
    "        X_train_batch = X_train[j: j+batch_size, :]\n",
    "        y_train_batch = y_trn[j: j+batch_size]\n",
    "        \n",
    "        train_input = Variable(torch.Tensor(X_train_batch.reshape((X_train_batch.shape[0], seq_length, -1))),\n",
    "                               requires_grad=True).to(device)\n",
    "        train_output = Variable(torch.from_numpy(y_train_batch)).to(device)\n",
    "        optimizer.zero_grad()                     \n",
    "        # Forward propagation\n",
    "        prediction = RNN(train_input)\n",
    "\n",
    "        # Calculating loss\n",
    "        loss = criterion(prediction, train_output.to(torch.long))\n",
    "        epoch_errors.append(loss)\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Training Loss: %1.5f\" % (i, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Training LSTM using skorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_s=3\n",
    "input_size = 2\n",
    "seq_length = X_train.shape[1]//input_size\n",
    "hidden_size = 3\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "model_lstm = NeuralNetClassifier(module = LSTM,\n",
    "                            module__input_size = input_size,\n",
    "                            module__hidden_size = hidden_size,\n",
    "                            module__num_layers = num_layers,\n",
    "                            module__seq_length  = seq_length,\n",
    "                            module__num_classes = num_classes,\n",
    "                            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                            criterion = nn.CrossEntropyLoss,\n",
    "                            optimizer = optim.Adam,\n",
    "                            lr = 0.008,\n",
    "                            callbacks=[('tr_acc', callbacks.EpochScoring('accuracy',\n",
    "                                                                         lower_is_better=False,\n",
    "                                                                         on_train=True,\n",
    "                                                                         name='train_acc'))],\n",
    "                            batch_size = 2000,\n",
    "                            max_epochs= 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training LSTM\n",
      "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -----------  ------------  -----------  ------------  ------\n",
      "      1       \u001b[36m0.3365\u001b[0m        \u001b[32m1.1039\u001b[0m       \u001b[35m0.3313\u001b[0m        \u001b[31m1.0987\u001b[0m  0.6560\n",
      "      2       \u001b[36m0.3374\u001b[0m        \u001b[32m1.0942\u001b[0m       \u001b[35m0.4233\u001b[0m        \u001b[31m1.0798\u001b[0m  0.3920\n",
      "      3       \u001b[36m0.4074\u001b[0m        \u001b[32m1.0742\u001b[0m       0.3954        \u001b[31m1.0597\u001b[0m  0.4590\n",
      "      4       0.3998        \u001b[32m1.0653\u001b[0m       0.4114        \u001b[31m1.0548\u001b[0m  0.4310\n",
      "      5       \u001b[36m0.4130\u001b[0m        \u001b[32m1.0599\u001b[0m       0.4160        \u001b[31m1.0490\u001b[0m  0.4850\n",
      "      6       \u001b[36m0.4150\u001b[0m        \u001b[32m1.0533\u001b[0m       0.4177        \u001b[31m1.0424\u001b[0m  0.4190\n",
      "      7       \u001b[36m0.4178\u001b[0m        \u001b[32m1.0474\u001b[0m       \u001b[35m0.4236\u001b[0m        \u001b[31m1.0367\u001b[0m  0.4730\n",
      "      8       0.4174        \u001b[32m1.0402\u001b[0m       0.4170        \u001b[31m1.0345\u001b[0m  0.4190\n",
      "      9       \u001b[36m0.4198\u001b[0m        \u001b[32m1.0357\u001b[0m       0.4173        1.0370  0.4150\n",
      "     10       0.4183        \u001b[32m1.0347\u001b[0m       0.4190        1.0385  0.4220\n",
      "     11       \u001b[36m0.4205\u001b[0m        \u001b[32m1.0344\u001b[0m       0.4210        \u001b[31m1.0279\u001b[0m  0.5210\n",
      "     12       \u001b[36m0.4275\u001b[0m        \u001b[32m1.0291\u001b[0m       \u001b[35m0.4243\u001b[0m        \u001b[31m1.0269\u001b[0m  0.3900\n",
      "     13       \u001b[36m0.4294\u001b[0m        \u001b[32m1.0262\u001b[0m       \u001b[35m0.4246\u001b[0m        1.0277  0.5030\n",
      "     14       \u001b[36m0.4316\u001b[0m        \u001b[32m1.0254\u001b[0m       0.4216        \u001b[31m1.0260\u001b[0m  0.4160\n",
      "     15       \u001b[36m0.4330\u001b[0m        \u001b[32m1.0238\u001b[0m       0.4216        \u001b[31m1.0228\u001b[0m  0.5130\n",
      "     16       \u001b[36m0.4349\u001b[0m        \u001b[32m1.0214\u001b[0m       0.4223        \u001b[31m1.0211\u001b[0m  0.4230\n",
      "     17       \u001b[36m0.4388\u001b[0m        \u001b[32m1.0197\u001b[0m       0.4230        \u001b[31m1.0200\u001b[0m  0.4420\n",
      "     18       \u001b[36m0.4403\u001b[0m        \u001b[32m1.0186\u001b[0m       \u001b[35m0.4250\u001b[0m        \u001b[31m1.0184\u001b[0m  0.5050\n",
      "     19       \u001b[36m0.4438\u001b[0m        \u001b[32m1.0172\u001b[0m       \u001b[35m0.4306\u001b[0m        \u001b[31m1.0165\u001b[0m  0.3990\n",
      "     20       \u001b[36m0.4473\u001b[0m        \u001b[32m1.0156\u001b[0m       \u001b[35m0.4313\u001b[0m        \u001b[31m1.0148\u001b[0m  0.5400\n",
      "     21       \u001b[36m0.4488\u001b[0m        \u001b[32m1.0141\u001b[0m       \u001b[35m0.4349\u001b[0m        \u001b[31m1.0139\u001b[0m  0.4570\n",
      "     22       \u001b[36m0.4503\u001b[0m        \u001b[32m1.0130\u001b[0m       \u001b[35m0.4416\u001b[0m        \u001b[31m1.0134\u001b[0m  0.4890\n",
      "     23       0.4501        \u001b[32m1.0122\u001b[0m       \u001b[35m0.4439\u001b[0m        \u001b[31m1.0134\u001b[0m  0.4350\n",
      "     24       \u001b[36m0.4509\u001b[0m        \u001b[32m1.0113\u001b[0m       \u001b[35m0.4449\u001b[0m        \u001b[31m1.0115\u001b[0m  0.4080\n",
      "     25       \u001b[36m0.4521\u001b[0m        \u001b[32m1.0095\u001b[0m       \u001b[35m0.4485\u001b[0m        \u001b[31m1.0100\u001b[0m  0.5020\n",
      "     26       0.4521        \u001b[32m1.0077\u001b[0m       \u001b[35m0.4495\u001b[0m        1.0101  0.4060\n",
      "     27       \u001b[36m0.4540\u001b[0m        \u001b[32m1.0063\u001b[0m       0.4489        \u001b[31m1.0092\u001b[0m  0.4660\n",
      "     28       \u001b[36m0.4543\u001b[0m        \u001b[32m1.0058\u001b[0m       0.4459        \u001b[31m1.0091\u001b[0m  0.4020\n",
      "     29       \u001b[36m0.4566\u001b[0m        \u001b[32m1.0049\u001b[0m       0.4446        \u001b[31m1.0088\u001b[0m  0.5520\n",
      "     30       0.4563        \u001b[32m1.0043\u001b[0m       0.4422        1.0092  0.4070\n",
      "     31       \u001b[36m0.4584\u001b[0m        \u001b[32m1.0041\u001b[0m       0.4422        1.0090  0.5030\n",
      "     32       0.4556        \u001b[32m1.0038\u001b[0m       0.4406        1.0093  0.4400\n",
      "     33       0.4567        \u001b[32m1.0036\u001b[0m       0.4399        1.0094  0.5220\n",
      "     34       0.4567        \u001b[32m1.0034\u001b[0m       0.4396        1.0101  0.4260\n",
      "     35       \u001b[36m0.4590\u001b[0m        \u001b[32m1.0034\u001b[0m       0.4382        1.0103  0.4390\n",
      "     36       \u001b[36m0.4594\u001b[0m        \u001b[32m1.0031\u001b[0m       0.4369        1.0111  0.5440\n",
      "     37       0.4594        1.0031       0.4353        1.0115  0.4900\n",
      "     38       \u001b[36m0.4619\u001b[0m        \u001b[32m1.0027\u001b[0m       0.4349        1.0123  0.5330\n",
      "     39       0.4617        \u001b[32m1.0027\u001b[0m       0.4346        1.0129  0.4580\n",
      "     40       0.4618        \u001b[32m1.0022\u001b[0m       0.4329        1.0137  0.5230\n",
      "     41       \u001b[36m0.4625\u001b[0m        1.0023       0.4346        1.0144  0.4760\n",
      "     42       0.4619        \u001b[32m1.0017\u001b[0m       0.4319        1.0151  0.5140\n",
      "     43       \u001b[36m0.4648\u001b[0m        1.0018       0.4323        1.0161  0.4690\n",
      "     44       0.4642        \u001b[32m1.0012\u001b[0m       0.4356        1.0169  0.4590\n",
      "     45       \u001b[36m0.4658\u001b[0m        1.0013       0.4333        1.0176  0.4160\n",
      "     46       0.4645        \u001b[32m1.0008\u001b[0m       0.4363        1.0184  0.5300\n",
      "     47       \u001b[36m0.4663\u001b[0m        \u001b[32m1.0006\u001b[0m       0.4346        1.0186  0.4080\n",
      "     48       0.4658        \u001b[32m1.0001\u001b[0m       0.4389        1.0197  0.5000\n",
      "     49       \u001b[36m0.4673\u001b[0m        \u001b[32m1.0000\u001b[0m       0.4396        1.0200  0.5200\n",
      "     50       \u001b[36m0.4692\u001b[0m        \u001b[32m0.9995\u001b[0m       0.4412        1.0210  0.4990\n",
      "     51       0.4687        0.9996       0.4412        1.0216  0.4360\n",
      "     52       0.4687        \u001b[32m0.9991\u001b[0m       0.4419        1.0222  0.4260\n",
      "     53       0.4680        \u001b[32m0.9990\u001b[0m       0.4409        1.0226  0.4930\n",
      "     54       \u001b[36m0.4693\u001b[0m        \u001b[32m0.9985\u001b[0m       0.4412        1.0227  0.4340\n",
      "     55       \u001b[36m0.4701\u001b[0m        \u001b[32m0.9981\u001b[0m       0.4412        1.0227  0.4990\n",
      "     56       \u001b[36m0.4710\u001b[0m        \u001b[32m0.9977\u001b[0m       0.4422        1.0229  0.4530\n",
      "     57       0.4702        \u001b[32m0.9975\u001b[0m       0.4422        1.0235  0.5810\n",
      "     58       \u001b[36m0.4718\u001b[0m        \u001b[32m0.9971\u001b[0m       0.4429        1.0236  0.5410\n",
      "     59       0.4716        \u001b[32m0.9968\u001b[0m       0.4426        1.0243  0.4910\n",
      "     60       0.4716        \u001b[32m0.9965\u001b[0m       0.4426        1.0246  0.5550\n",
      "     61       \u001b[36m0.4721\u001b[0m        \u001b[32m0.9962\u001b[0m       0.4436        1.0250  0.4500\n",
      "     62       \u001b[36m0.4728\u001b[0m        \u001b[32m0.9960\u001b[0m       0.4442        1.0255  0.5710\n",
      "     63       0.4720        \u001b[32m0.9958\u001b[0m       0.4442        1.0258  0.4330\n",
      "     64       \u001b[36m0.4729\u001b[0m        \u001b[32m0.9956\u001b[0m       0.4432        1.0262  0.4740\n",
      "     65       \u001b[36m0.4741\u001b[0m        \u001b[32m0.9953\u001b[0m       0.4439        1.0265  0.4700\n",
      "     66       \u001b[36m0.4756\u001b[0m        \u001b[32m0.9950\u001b[0m       0.4452        1.0268  0.4940\n",
      "     67       \u001b[36m0.4764\u001b[0m        \u001b[32m0.9948\u001b[0m       0.4449        1.0272  0.4230\n",
      "     68       0.4761        \u001b[32m0.9945\u001b[0m       0.4439        1.0274  0.4010\n",
      "     69       \u001b[36m0.4775\u001b[0m        \u001b[32m0.9944\u001b[0m       0.4429        1.0279  0.4030\n",
      "     70       0.4759        \u001b[32m0.9941\u001b[0m       0.4439        1.0280  0.4880\n"
     ]
    }
   ],
   "source": [
    "trainer.train_lstm(LSTM,params={\n",
    "    'input_size': input_size,\n",
    "    'seq_length': X_train.shape[1]//input_size,\n",
    "    'hidden_size': 3,\n",
    "    'num_layers': 2,\n",
    "    'num_classes': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training LSTM-CNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training LSTM-CNN\n",
      "  epoch    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -----------  ------------  -----------  ------------  ------\n",
      "      1       \u001b[36m0.3431\u001b[0m        \u001b[32m1.0977\u001b[0m       \u001b[35m0.3569\u001b[0m        \u001b[31m1.0927\u001b[0m  2.1831\n",
      "      2       \u001b[36m0.3756\u001b[0m        \u001b[32m1.0884\u001b[0m       \u001b[35m0.4193\u001b[0m        \u001b[31m1.0757\u001b[0m  0.5420\n",
      "      3       \u001b[36m0.4012\u001b[0m        \u001b[32m1.0709\u001b[0m       0.4153        \u001b[31m1.0510\u001b[0m  0.6190\n",
      "      4       \u001b[36m0.4036\u001b[0m        \u001b[32m1.0529\u001b[0m       \u001b[35m0.4220\u001b[0m        \u001b[31m1.0346\u001b[0m  0.5529\n",
      "      5       \u001b[36m0.4108\u001b[0m        \u001b[32m1.0423\u001b[0m       \u001b[35m0.4256\u001b[0m        \u001b[31m1.0299\u001b[0m  0.5990\n",
      "      6       \u001b[36m0.4154\u001b[0m        \u001b[32m1.0361\u001b[0m       0.4250        \u001b[31m1.0242\u001b[0m  0.5440\n",
      "      7       0.4140        \u001b[32m1.0317\u001b[0m       0.4220        \u001b[31m1.0215\u001b[0m  0.5340\n",
      "      8       0.4154        \u001b[32m1.0295\u001b[0m       0.4250        \u001b[31m1.0198\u001b[0m  0.6010\n",
      "      9       0.4153        \u001b[32m1.0279\u001b[0m       0.4250        \u001b[31m1.0186\u001b[0m  0.5510\n",
      "     10       \u001b[36m0.4178\u001b[0m        \u001b[32m1.0269\u001b[0m       0.4233        \u001b[31m1.0178\u001b[0m  0.5920\n",
      "     11       0.4160        \u001b[32m1.0258\u001b[0m       0.4243        \u001b[31m1.0170\u001b[0m  0.5270\n",
      "     12       0.4157        \u001b[32m1.0249\u001b[0m       0.4256        \u001b[31m1.0163\u001b[0m  0.5480\n",
      "     13       \u001b[36m0.4178\u001b[0m        \u001b[32m1.0240\u001b[0m       \u001b[35m0.4266\u001b[0m        \u001b[31m1.0156\u001b[0m  0.6200\n",
      "     14       \u001b[36m0.4198\u001b[0m        \u001b[32m1.0235\u001b[0m       \u001b[35m0.4276\u001b[0m        \u001b[31m1.0148\u001b[0m  0.5280\n",
      "     15       0.4190        \u001b[32m1.0222\u001b[0m       0.4273        \u001b[31m1.0138\u001b[0m  0.6070\n",
      "     16       \u001b[36m0.4216\u001b[0m        \u001b[32m1.0207\u001b[0m       \u001b[35m0.4290\u001b[0m        \u001b[31m1.0128\u001b[0m  0.5410\n",
      "     17       \u001b[36m0.4228\u001b[0m        \u001b[32m1.0200\u001b[0m       0.4286        \u001b[31m1.0121\u001b[0m  0.5280\n",
      "     18       \u001b[36m0.4229\u001b[0m        \u001b[32m1.0189\u001b[0m       0.4236        \u001b[31m1.0116\u001b[0m  0.6130\n",
      "     19       \u001b[36m0.4298\u001b[0m        \u001b[32m1.0168\u001b[0m       0.4276        1.0119  0.5430\n",
      "     20       0.4296        \u001b[32m1.0158\u001b[0m       \u001b[35m0.4306\u001b[0m        1.0134  0.5290\n",
      "     21       \u001b[36m0.4341\u001b[0m        \u001b[32m1.0156\u001b[0m       \u001b[35m0.4382\u001b[0m        \u001b[31m1.0104\u001b[0m  0.6170\n",
      "     22       0.4325        1.0158       0.4343        1.0118  0.5590\n",
      "     23       \u001b[36m0.4386\u001b[0m        \u001b[32m1.0125\u001b[0m       \u001b[35m0.4422\u001b[0m        \u001b[31m1.0096\u001b[0m  0.6660\n",
      "     24       \u001b[36m0.4432\u001b[0m        \u001b[32m1.0090\u001b[0m       0.4359        \u001b[31m1.0090\u001b[0m  0.5740\n",
      "     25       \u001b[36m0.4469\u001b[0m        \u001b[32m1.0072\u001b[0m       0.4369        1.0101  0.5760\n",
      "     26       \u001b[36m0.4506\u001b[0m        \u001b[32m1.0052\u001b[0m       0.4369        1.0090  0.6410\n",
      "     27       \u001b[36m0.4546\u001b[0m        \u001b[32m1.0031\u001b[0m       0.4346        1.0103  0.6020\n",
      "     28       \u001b[36m0.4573\u001b[0m        1.0039       0.4326        1.0107  0.5560\n",
      "     29       0.4561        1.0041       0.4316        1.0106  0.6130\n",
      "     30       0.4553        \u001b[32m1.0026\u001b[0m       0.4356        \u001b[31m1.0078\u001b[0m  0.5740\n",
      "     31       0.4570        \u001b[32m0.9990\u001b[0m       \u001b[35m0.4439\u001b[0m        1.0090  0.5440\n",
      "     32       \u001b[36m0.4609\u001b[0m        \u001b[32m0.9976\u001b[0m       \u001b[35m0.4456\u001b[0m        1.0083  0.6200\n",
      "     33       \u001b[36m0.4648\u001b[0m        \u001b[32m0.9966\u001b[0m       0.4422        1.0086  0.5440\n",
      "     34       \u001b[36m0.4669\u001b[0m        \u001b[32m0.9957\u001b[0m       0.4409        \u001b[31m1.0073\u001b[0m  0.5320\n",
      "     35       \u001b[36m0.4692\u001b[0m        \u001b[32m0.9949\u001b[0m       0.4436        1.0097  0.5980\n",
      "     36       0.4673        \u001b[32m0.9943\u001b[0m       0.4422        1.0090  0.5990\n",
      "     37       0.4689        \u001b[32m0.9929\u001b[0m       0.4402        1.0100  0.5820\n",
      "     38       \u001b[36m0.4701\u001b[0m        0.9932       0.4396        1.0104  0.6170\n",
      "     39       0.4696        0.9943       0.4406        1.0093  0.5430\n",
      "     40       \u001b[36m0.4716\u001b[0m        \u001b[32m0.9918\u001b[0m       \u001b[35m0.4459\u001b[0m        \u001b[31m1.0062\u001b[0m  0.5570\n",
      "     41       \u001b[36m0.4748\u001b[0m        \u001b[32m0.9906\u001b[0m       \u001b[35m0.4482\u001b[0m        1.0067  0.6170\n",
      "     42       0.4741        \u001b[32m0.9897\u001b[0m       \u001b[35m0.4492\u001b[0m        1.0072  0.5460\n",
      "     43       0.4743        \u001b[32m0.9890\u001b[0m       \u001b[35m0.4495\u001b[0m        1.0086  0.5499\n",
      "     44       0.4748        \u001b[32m0.9880\u001b[0m       0.4459        1.0089  0.5970\n",
      "     45       \u001b[36m0.4763\u001b[0m        \u001b[32m0.9868\u001b[0m       0.4469        1.0099  0.5450\n",
      "     46       \u001b[36m0.4780\u001b[0m        \u001b[32m0.9861\u001b[0m       \u001b[35m0.4509\u001b[0m        1.0087  0.5540\n",
      "     47       \u001b[36m0.4793\u001b[0m        \u001b[32m0.9850\u001b[0m       0.4499        1.0090  0.6270\n",
      "     48       \u001b[36m0.4800\u001b[0m        \u001b[32m0.9840\u001b[0m       \u001b[35m0.4539\u001b[0m        1.0080  0.6230\n",
      "     49       \u001b[36m0.4829\u001b[0m        \u001b[32m0.9835\u001b[0m       \u001b[35m0.4562\u001b[0m        1.0089  0.5860\n",
      "     50       \u001b[36m0.4845\u001b[0m        \u001b[32m0.9829\u001b[0m       \u001b[35m0.4578\u001b[0m        1.0086  0.6100\n",
      "     51       0.4820        0.9835       0.4539        1.0153  0.6280\n",
      "     52       0.4770        0.9992       0.4562        1.0091  0.5970\n",
      "     53       0.4839        0.9879       0.4479        1.0103  0.5860\n",
      "     54       0.4809        0.9845       0.4529        1.0098  0.5830\n",
      "     55       0.4829        0.9838       0.4542        1.0138  0.7750\n",
      "     56       \u001b[36m0.4855\u001b[0m        \u001b[32m0.9812\u001b[0m       0.4558        1.0153  0.5820\n",
      "     57       0.4831        0.9814       0.4565        1.0128  0.5760\n",
      "     58       \u001b[36m0.4872\u001b[0m        \u001b[32m0.9810\u001b[0m       0.4522        1.0148  0.5500\n",
      "     59       0.4849        \u001b[32m0.9795\u001b[0m       0.4575        1.0145  0.6090\n",
      "     60       \u001b[36m0.4900\u001b[0m        \u001b[32m0.9787\u001b[0m       0.4572        1.0153  0.5730\n",
      "     61       0.4868        \u001b[32m0.9778\u001b[0m       0.4542        1.0170  0.5410\n",
      "     62       0.4885        \u001b[32m0.9772\u001b[0m       0.4529        1.0183  0.6190\n",
      "     63       0.4888        \u001b[32m0.9769\u001b[0m       0.4532        1.0162  0.5620\n",
      "     64       \u001b[36m0.4906\u001b[0m        \u001b[32m0.9762\u001b[0m       0.4542        1.0179  0.5390\n",
      "     65       \u001b[36m0.4923\u001b[0m        \u001b[32m0.9755\u001b[0m       0.4485        1.0183  0.5840\n",
      "     66       0.4922        \u001b[32m0.9748\u001b[0m       0.4515        1.0178  0.6260\n",
      "     67       \u001b[36m0.4941\u001b[0m        0.9758       0.4495        1.0200  0.5400\n",
      "     68       0.4922        0.9753       0.4456        1.0230  0.5430\n",
      "     69       0.4894        0.9757       0.4469        1.0207  0.5370\n",
      "     70       0.4933        0.9782       0.4515        1.0213  0.6160\n",
      "     71       \u001b[36m0.4969\u001b[0m        \u001b[32m0.9731\u001b[0m       0.4472        1.0209  0.5460\n",
      "     72       0.4963        \u001b[32m0.9711\u001b[0m       0.4422        1.0243  0.5590\n",
      "     73       0.4926        0.9716       0.4412        1.0340  0.5960\n",
      "     74       0.4900        0.9738       0.4416        1.0263  0.5310\n",
      "     75       0.4902        0.9764       0.4442        1.0257  0.5600\n",
      "     76       \u001b[36m0.4973\u001b[0m        \u001b[32m0.9685\u001b[0m       0.4439        1.0218  0.6370\n",
      "     77       \u001b[36m0.4993\u001b[0m        0.9693       0.4316        1.0308  0.5540\n",
      "     78       0.4991        \u001b[32m0.9677\u001b[0m       0.4396        1.0320  0.5780\n",
      "     79       0.4988        \u001b[32m0.9670\u001b[0m       0.4376        1.0309  0.5800\n",
      "     80       \u001b[36m0.5007\u001b[0m        \u001b[32m0.9646\u001b[0m       0.4353        1.0304  0.6340\n",
      "     81       \u001b[36m0.5029\u001b[0m        \u001b[32m0.9622\u001b[0m       0.4373        1.0321  0.5610\n",
      "     82       \u001b[36m0.5072\u001b[0m        0.9623       0.4366        1.0341  0.5610\n",
      "     83       0.5049        \u001b[32m0.9609\u001b[0m       0.4386        1.0366  0.6410\n",
      "     84       0.5034        0.9635       0.4366        1.0395  0.5409\n",
      "     85       0.5045        \u001b[32m0.9609\u001b[0m       0.4309        1.0418  0.5551\n",
      "     86       \u001b[36m0.5079\u001b[0m        \u001b[32m0.9608\u001b[0m       0.4333        1.0407  0.5590\n",
      "     87       0.5068        0.9611       0.4359        1.0450  0.6060\n",
      "     88       0.5079        \u001b[32m0.9591\u001b[0m       0.4353        1.0482  0.5420\n",
      "     89       0.5042        0.9609       0.4343        1.0507  0.5440\n",
      "     90       0.5070        0.9602       0.4326        1.0469  0.6230\n"
     ]
    }
   ],
   "source": [
    "trainer.train_rcnn(ConvRNN,params={\n",
    "    'hidden_size': 30,\n",
    "    'out_channels': 30,\n",
    "    'shape':(5,10)\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
