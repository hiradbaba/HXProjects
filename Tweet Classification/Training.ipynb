{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Stage (Step-by-Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_optimizer import ModelSelector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "import sklearn.tree\n",
    "import sklearn.svm\n",
    "import pickle\n",
    "from voting import VotingClassifier\n",
    "from trainer import VotingTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: ModelSelector class located in <i>\"model_optimizer.py\"</i> is responsible for applying grid/random hyper-parameter search based on given data and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/transform_data.csv').to_numpy()\n",
    "X, y = data[:, :50], data[:, -1:].astype(np.int32)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining grid parameters for hyperparameter search process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing trasformed data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15057, 50) (15057, 1) (3765, 50) (3765, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining grid parameters for hyperparameter search process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random'], 'max_depth': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 17, 19, 21,\n",
      "       23, 25, 27, 29, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90,\n",
      "       95]), 'min_samples_split': array([2, 3, 4, 5, 6, 7, 8, 9]), 'max_features': ['auto', 'sqrt', 'log2'], 'random_state': [0]}\n"
     ]
    }
   ],
   "source": [
    "DT = sklearn.tree.DecisionTreeClassifier(random_state=0)\n",
    "grid_params = {'criterion':['gini', 'entropy'],\n",
    "               'splitter':['best', 'random'],\n",
    "               'max_depth':np.hstack((np.arange(2, 15, 1), np.arange(15, 30, 2), np.arange(30, 100, 5))),\n",
    "               'min_samples_split':np.arange(2, 10, 1),\n",
    "               'max_features':['auto', 'sqrt', 'log2'],\n",
    "               'random_state':[0]\n",
    "              }\n",
    "print(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter search started\n",
      "Fitting 5 folds for each of 3360 candidates, totalling 16800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=4)]: Done 644 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=4)]: Done 1644 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=4)]: Done 3044 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 4844 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done 7044 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=4)]: Done 9644 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=4)]: Done 10448 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=4)]: Done 11298 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=4)]: Done 12248 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=4)]: Done 13298 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=4)]: Done 14448 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=4)]: Done 15698 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=4)]: Done 16800 out of 16800 | elapsed: 15.0min finished\n"
     ]
    }
   ],
   "source": [
    "model_selector = ModelSelector(DT, X_train, y_train)\n",
    "search_cv = model_selector.parameter_search(grid_params, False, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=8, max_features='auto', min_samples_split=7,\n",
      "                       random_state=0)\n",
      "0.42212939452198955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.51      0.53      4715\n",
      "           1       0.47      0.71      0.57      5374\n",
      "           2       0.51      0.28      0.36      4968\n",
      "\n",
      "    accuracy                           0.50     15057\n",
      "   macro avg       0.51      0.50      0.49     15057\n",
      "weighted avg       0.51      0.50      0.49     15057\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.46      0.48      1223\n",
      "           1       0.41      0.61      0.49      1320\n",
      "           2       0.37      0.21      0.27      1222\n",
      "\n",
      "    accuracy                           0.43      3765\n",
      "   macro avg       0.43      0.43      0.41      3765\n",
      "weighted avg       0.43      0.43      0.41      3765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_selector.best_estimator)\n",
    "print(search_cv.best_score_)\n",
    "print(model_selector.best_report(y_train, search_cv.best_estimator_.predict(X_train)))\n",
    "print(model_selector.best_report(y_test, search_cv.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/DT.pkl','wb') as file:\n",
    "    pickle.dump(model_selector.best_estimator, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.loadtxt('data/transform_data.csv', skiprows=1, delimiter=',')\n",
    "X, y = Data[:, :-1], Data[:, -1].astype('int')\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state = 0)\n",
    "trainer = VotingTrainer(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Linear kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter search started\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 out of  20 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(random_state=0)\n",
      "0.45274670166891384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.66      0.55      1223\n",
      "           1       0.48      0.53      0.50      1320\n",
      "           2       0.47      0.23      0.31      1222\n",
      "\n",
      "    accuracy                           0.47      3765\n",
      "   macro avg       0.47      0.47      0.45      3765\n",
      "weighted avg       0.47      0.47      0.45      3765\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "svm = sklearn.svm.LinearSVC(random_state = 0)\n",
    "c = np.logspace(-1, 2, 4)\n",
    "grid_params = [{'C':c}]\n",
    "model_selector = ModelSelector(svm, X_train, y_train)\n",
    "search_cv = model_selector.parameter_search(grid_params, False, n_jobs=6)\n",
    "print(model_selector.best_estimator)\n",
    "print(search_cv.best_score_)\n",
    "print(model_selector.best_report(y_test, search_cv.best_estimator_.predict(X_test)))\n",
    "with open('models/LinearSVM.pkl','wb') as file:\n",
    "    pickle.dump(model_selector.best_estimator, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Polynomial kernel*\n",
    "For some reason which was not detected the training process gets stuck for days. Therefore, the Polygnomial svm has not been trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter search started\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed: 17.1min\n"
     ]
    }
   ],
   "source": [
    "svm = sklearn.svm.SVC(random_state = 0)\n",
    "df_shape = ['ovo']\n",
    "c = np.logspace(-1, 2, 4)\n",
    "grid_params = [{'kernel':['poly'], 'C':c, 'degree':[2], 'coef0':np.logspace(0, 2, 3), 'decision_function_shape':df_shape}]\n",
    "model_selector = ModelSelector(svm, X_train, y_train)\n",
    "search_cv = model_selector.parameter_search(grid_params, False, n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *RBF kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter search started\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=6)]: Done  80 out of  80 | elapsed: 23.2min finished\n"
     ]
    }
   ],
   "source": [
    "svm = sklearn.svm.SVC(random_state = 0)\n",
    "c = np.logspace(-1, 2, 4)\n",
    "grid_params = [{'kernel':['rbf'], 'gamma':np.logspace(-2, 1, 4), 'C':c}]\n",
    "model_selector = ModelSelector(svm, X_train, y_train)\n",
    "search_cv = model_selector.parameter_search(grid_params, False, n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(decision_function_shape='ovo', gamma=0.01, random_state=0)\n",
      "0.4797109359528563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.53      0.58      1223\n",
      "           1       0.46      0.65      0.54      1320\n",
      "           2       0.45      0.32      0.38      1222\n",
      "\n",
      "    accuracy                           0.51      3765\n",
      "   macro avg       0.51      0.50      0.50      3765\n",
      "weighted avg       0.51      0.51      0.50      3765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_selector.best_estimator)\n",
    "print(search_cv.best_score_)\n",
    "print(model_selector.best_report(y_test, search_cv.best_estimator_.predict(X_test)))\n",
    "with open('models/svm.pkl','wb') as file:\n",
    "    pickle.dump(model_selector.best_estimator, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [10, 12, 15, 17, 20],\n",
       " 'max_features': ['auto', 'sqrt', 'log2'],\n",
       " 'max_depth': [2, 5, 8, 11, 14, 17, 20, 23, 26, 30],\n",
       " 'min_samples_split': [2, 5],\n",
       " 'bootstrap': [True, False],\n",
       " 'criterion': ['gini', 'entropy']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest=RandomForestClassifier(random_state=0)\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 20, num = 5)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(2, 30, num = 10)]\n",
    "min_samples_split = [2, 5]\n",
    "bootstrap = [True, False]\n",
    "criterion=['gini',\"entropy\"]\n",
    "grid_params = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "               'bootstrap': bootstrap,\n",
    "              'criterion':criterion}\n",
    "grid_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter search started\n",
      "Fitting 5 folds for each of 1200 candidates, totalling 6000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=10)]: Done 2430 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=10)]: Done 3180 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=10)]: Done 4030 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=10)]: Done 4980 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=10)]: Done 6000 out of 6000 | elapsed: 32.4min finished\n"
     ]
    }
   ],
   "source": [
    "model_selector = ModelSelector(random_forest,X_train,y_train)\n",
    "search_cv = model_selector.parameter_search(grid_params,False,n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=14,\n",
      "                       max_features='log2', n_estimators=20, random_state=0)\n",
      "0.4558673310742417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4715\n",
      "           1       0.99      0.99      0.99      5374\n",
      "           2       0.99      0.99      0.99      4968\n",
      "\n",
      "    accuracy                           0.99     15057\n",
      "   macro avg       0.99      0.99      0.99     15057\n",
      "weighted avg       0.99      0.99      0.99     15057\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.43      0.51      1223\n",
      "           1       0.43      0.55      0.48      1320\n",
      "           2       0.39      0.40      0.39      1222\n",
      "\n",
      "    accuracy                           0.46      3765\n",
      "   macro avg       0.49      0.46      0.46      3765\n",
      "weighted avg       0.48      0.46      0.46      3765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_selector.best_estimator)\n",
    "print(search_cv.best_score_)\n",
    "print(model_selector.best_report(y_train,search_cv.best_estimator_.predict(X_train)))\n",
    "print(model_selector.best_report(y_test,search_cv.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/RF.pkl','wb') as file:\n",
    "    pickle.dump(model_selector.best_estimator, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter search started\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 107.1min finished\n"
     ]
    }
   ],
   "source": [
    "adaboost_clf = sklearn.ensemble.AdaBoostClassifier(random_state=0)\n",
    "estimators_range = np.arange(500,1001,100)\n",
    "lr_range = np.logspace(-2,2,4)\n",
    "algorithms = ['SAMME','SAMME.R']\n",
    "base_estimator = [ sklearn.tree.DecisionTreeClassifier(max_depth=i,random_state=0) for i in range(1,4)\n",
    "                 ]\n",
    "dist_params = {'n_estimators':estimators_range,\n",
    "               'learning_rate':[0.01],\n",
    "               'algorithm':algorithms,\n",
    "               'base_estimator': base_estimator\n",
    "            }\n",
    "model_selector = ModelSelector(adaboost_clf,X_train,y_train)\n",
    "search_cv = model_selector.parameter_search(dist_params,False,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
      "                                                         random_state=0),\n",
      "                   learning_rate=0.01, n_estimators=900, random_state=0)\n",
      "0.4664948530906816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.53      0.59      4715\n",
      "           1       0.51      0.69      0.59      5374\n",
      "           2       0.49      0.40      0.44      4968\n",
      "\n",
      "    accuracy                           0.54     15057\n",
      "   macro avg       0.56      0.54      0.54     15057\n",
      "weighted avg       0.55      0.54      0.54     15057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_selector.best_estimator)\n",
    "print(search_cv.best_score_)\n",
    "print(model_selector.best_report(y_train,search_cv.best_estimator_.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.46      0.52      1223\n",
      "           1       0.46      0.62      0.53      1320\n",
      "           2       0.42      0.35      0.38      1222\n",
      "\n",
      "    accuracy                           0.48      3765\n",
      "   macro avg       0.49      0.48      0.48      3765\n",
      "weighted avg       0.49      0.48      0.48      3765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_selector.best_report(y_test,search_cv.best_estimator_.predict(X_test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/adaboost.pkl','wb') as file:\n",
    "    pickle.dump(model_selector.best_estimator,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper parameter search started\n",
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 57.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 87.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 125.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 153.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 171.1min finished\n"
     ]
    }
   ],
   "source": [
    "train_data = np.loadtxt('training.csv',skiprows=1,delimiter=',')\n",
    "test_data = np.loadtxt('test.csv',skiprows=1,delimiter=',')\n",
    "X_train,y_train = train_data[:,:-1],train_data[:,-1].astype('int')\n",
    "X_tset,y_test = test_data[:,:-1],test_data[:,-1].astype('int')\n",
    "\n",
    "mlp_clf3 = sklearn.neural_network.MLPClassifier(random_state=0)\n",
    "\n",
    "\n",
    "dist_params3 = {\n",
    "    'hidden_layer_sizes': [(25,12,6),(25,10),(25,),(60,30,10),\n",
    "                            (50,50,25,25),(25,10,5),(50,10,3),(40,25,10,6)],\n",
    "    'activation': ['tanh', 'relu','logistic'],\n",
    "    'solver': ['sgd', 'adam','lbfgs'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'max_iter':[500,1000]\n",
    "            }\n",
    "\n",
    "model_selector_3 = ModelSelector(mlp_clf3,X_train,y_train)\n",
    "search_cv_3 = model_selector_3.parameter_search(dist_params3,False,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.57      4715\n",
      "           1       0.52      0.58      0.55      5374\n",
      "           2       0.46      0.41      0.44      4968\n",
      "\n",
      "    accuracy                           0.52     15057\n",
      "   macro avg       0.52      0.52      0.52     15057\n",
      "weighted avg       0.52      0.52      0.52     15057\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56      1223\n",
      "           1       0.50      0.56      0.52      1320\n",
      "           2       0.42      0.38      0.40      1222\n",
      "\n",
      "    accuracy                           0.50      3765\n",
      "   macro avg       0.50      0.50      0.50      3765\n",
      "weighted avg       0.50      0.50      0.50      3765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_selector_3.best_report(y_train,model_selector_3.best_estimator.predict(X_train)))\n",
    "print(model_selector_3.best_report(y_test,model_selector_3.best_estimator.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making random weight matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       brand       0.76      0.75      0.75      4715\n",
      "      female       0.71      0.83      0.76      5374\n",
      "        male       0.81      0.66      0.73      4968\n",
      "\n",
      "    accuracy                           0.75     15057\n",
      "   macro avg       0.76      0.75      0.75     15057\n",
      "weighted avg       0.76      0.75      0.75     15057\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       brand       0.63      0.53      0.58      1223\n",
      "      female       0.46      0.65      0.54      1320\n",
      "        male       0.45      0.32      0.38      1222\n",
      "\n",
      "    accuracy                           0.51      3765\n",
      "   macro avg       0.51      0.50      0.50      3765\n",
      "weighted avg       0.51      0.51      0.50      3765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('models/adaboost.pkl','rb') as f1:\n",
    "    adaboost_clf = pickle.load(f1)\n",
    "with open('models/DT.pkl','rb') as f:\n",
    "    dt_clf = pickle.load(f)\n",
    "with open('models/MLPClassifier.pkl','rb') as f2:\n",
    "    mlp_clf = pickle.load(f2)\n",
    "with open('models/svm.pkl','rb') as f3:\n",
    "    svm_clf = pickle.load(f3)\n",
    "\n",
    "models = [('adaboost',adaboost_clf),('mlp',mlp_clf),('dt',dt_clf),('svm',svm_clf)]  \n",
    "trainer.train_vote_clf(models,100,1500)\n",
    "print(trainer.get_report())\n",
    "print(trainer.get_test_report(X_test,trainer.le.inverse_transform(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
